{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.datasets import imdb\n",
    "\n",
    "max_words_in_voc = 10000 \n",
    "\n",
    "special_tokens = {0: '<pad>', \n",
    "                  1: '<start>', \n",
    "                  2: '<oov>'}\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(\n",
    "                                          path=\"imdb.npz\",\n",
    "                                          num_words=max_words_in_voc, # maximum number of indexed word, None = all\n",
    "                                          skip_top=0, # skip n words with the highest occurance count\n",
    "                                          maxlen=None, # truncate examples longer then N words\n",
    "                                          seed=113, # random seed\n",
    "                                          start_char=1, # index to be used for <start> token\n",
    "                                          oov_char=2, # index to be used for unindexed words\n",
    "                                          index_from=len(special_tokens)) # add `index_from` to all inidcies for regular words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:  [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "\n",
      "Y:  1\n",
      "\n",
      "Sentance length distribution:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x182c3f8940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"X: \", x_train[0])\n",
    "print()\n",
    "print(\"Y: \", y_train[0])\n",
    "print()\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "print(\"Sentance length distribution:\")\n",
    "sent_lens = list(map(len, x_train))\n",
    "lens = plt.hist(sent_lens, bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size = 10000 tokens\n"
     ]
    }
   ],
   "source": [
    "word2ind = imdb.get_word_index()\n",
    "\n",
    "ind2word = {ind + len(special_tokens): word for word, ind in word2ind.items()} # each index is shifted by 3, as we stated in the load_imdb function\n",
    "ind2word.update(special_tokens)\n",
    "voc_size = min(max_words_in_voc, len(ind2word)) # maximum word index in our dataset + \n",
    "\n",
    "print(\"Vocabulary size = %d tokens\" % voc_size)\n",
    "\n",
    "def inds2text(inds_list):\n",
    "    return ' '.join(map(ind2word.get, inds_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<start> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <oov> is an amazing actor and now the same being director <oov> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <oov> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <oov> to the two little boy's that played the <oov> of norman and paul they were just brilliant children are often left out of the <oov> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sentance_inds = x_train[0]\n",
    "inds2text(sample_sentance_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example lengths:  [218, 189, 141, 550, 147]\n"
     ]
    }
   ],
   "source": [
    "def get_sample_batch(batch_size):\n",
    "    batch_X = x_train[:batch_size]\n",
    "    return batch_X\n",
    "\n",
    "batch_X = get_sample_batch(5)\n",
    "batch_lens = [len(sent_inds) for sent_inds in batch_X]\n",
    "print(\"Example lengths: \", batch_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X:  (5, 100)\n",
      "\n",
      "Sample example:  [   1  194 1153  194 8255   78  228    5    6 1463 4369 5012  134   26\n",
      "    4  715    8  118 1634   14  394   20   13  119  954  189  102    5\n",
      "  207  110 3103   21   14   69  188    8   30   23    7    4  249  126\n",
      "   93    4  114    9 2300 1523    5  647    4  116    9   35 8163    4\n",
      "  229    9  340 1322    4  118    9    4  130 4901   19    4 1002    5\n",
      "   89   29  952   46   37    4  455    9   45   43   38 1543 1905  398\n",
      "    4 1649   26 6853    5  163   11 3215    2    4 1153    9  194  775\n",
      "    7 8255]\n",
      "\n",
      "Its text:  <start> big hair big boobs bad music and a giant safety pin these are the words to best describe this terrible movie i love cheesy horror movies and i've seen hundreds but this had got to be on of the worst ever made the plot is paper thin and ridiculous the acting is an abomination the script is completely laughable the best is the end showdown with the cop and how he worked out who the killer is it's just so damn terribly written the clothes are sickening and funny in equal <oov> the hair is big lots of boobs\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "def pad_batch(batch, max_seq_len):\n",
    "    batch_padded = sequence.pad_sequences(batch, \n",
    "                                          maxlen=max_seq_len, # maximum length of the example\n",
    "                                          padding='post', # from which end to pad short examples\n",
    "                                          truncating='post') # from which end to truncate long examples\n",
    "    return batch_padded\n",
    "\n",
    "batch_padded = pad_batch(batch_X, 100)\n",
    "print(\"Shape of X: \", batch_padded.shape)\n",
    "print()\n",
    "print(\"Sample example: \", batch_padded[1])\n",
    "print()\n",
    "print(\"Its text: \", inds2text(batch_padded[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_parameters = dict()\n",
    "hyper_parameters['bs'] = 5\n",
    "hyper_parameters['max_len'] = 100\n",
    "hyper_parameters['voc_size'] = voc_size\n",
    "\n",
    "network_dict = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ph_X shape: (5, 100)\n"
     ]
    }
   ],
   "source": [
    "def create_placeholders(hyper_parameters,\n",
    "                        network_dict):\n",
    "    network_dict['ph_X'] = tf.placeholder(shape=(hyper_parameters['bs'],\n",
    "                                                 hyper_parameters['max_len']), \n",
    "                                          dtype=tf.int32, \n",
    "                                          name=\"text_input\") # bs, max_len\n",
    "\n",
    "create_placeholders(hyper_parameters, network_dict)\n",
    "\n",
    "# Check that placeholder have the expected shape\n",
    "print(\"ph_X shape:\", network_dict['ph_X'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of `word_embeddings` tensor: (5, 100, 256)\n"
     ]
    }
   ],
   "source": [
    "hyper_parameters['emb_size'] = 256\n",
    "\n",
    "def create_embedding_layer(hyper_parameters, \n",
    "                           network_dict):\n",
    "    var_embs = tf.get_variable('var_embs', shape=[hyper_parameters['voc_size'], \n",
    "                                                                  hyper_parameters['emb_size']],\n",
    "                                               dtype=tf.float32,\n",
    "                                               initializer=tf.random_uniform_initializer(-1, 1),\n",
    "                                               trainable=True)\n",
    "    \n",
    "    \n",
    "    network_dict['word_embeddings'] = \\\n",
    "        tf.nn.embedding_lookup(var_embs, network_dict['ph_X']) # (bs, max_len, emb_size)\n",
    "\n",
    "create_embedding_layer(hyper_parameters, network_dict)\n",
    "\n",
    "# Check that tensor have expected shape\n",
    "print(\"Shape of `word_embeddings` tensor:\", network_dict['word_embeddings'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_parameters['cell_size'] = 512\n",
    "\n",
    "def create_lstm_variables(hyper_parameters, network_dict):\n",
    "    emb_size = hyper_parameters['emb_size']\n",
    "    cell_size = hyper_parameters['cell_size']\n",
    "    # Input weights + bias\n",
    "    network_dict['W_i'] = tf.get_variable('W_i', \n",
    "                          shape=[emb_size + cell_size, cell_size], \n",
    "                          dtype=tf.float32,\n",
    "                          initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.1))\n",
    "\n",
    "\n",
    "    network_dict['b_i'] = tf.get_variable('b_i', \n",
    "                          shape=[cell_size,], \n",
    "                          dtype=tf.float32,\n",
    "                          initializer=tf.zeros_initializer())\n",
    "\n",
    "    # Forget weights + bias\n",
    "    network_dict['W_f'] = tf.get_variable('W_f', \n",
    "                          shape=[emb_size + cell_size, cell_size], \n",
    "                          dtype=tf.float32,\n",
    "                          initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.1))\n",
    "\n",
    "    network_dict['b_f'] = tf.get_variable('b_f', \n",
    "                          shape=[cell_size,], \n",
    "                          dtype=tf.float32,\n",
    "                          initializer=tf.zeros_initializer())\n",
    "    \n",
    "    # Memory weights + bias\n",
    "    network_dict['W_c'] = tf.get_variable('W_c', \n",
    "                          shape=[emb_size + cell_size, cell_size], \n",
    "                          dtype=tf.float32,\n",
    "                          initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.1))\n",
    "\n",
    "    network_dict['b_c'] = tf.get_variable('b_c', \n",
    "                          shape=[cell_size, ], \n",
    "                          dtype=tf.float32,\n",
    "                          initializer=tf.zeros_initializer())\n",
    "    \n",
    "    # Output weights + bias\n",
    "    network_dict['W_o'] = tf.get_variable('W_o', \n",
    "                          shape=[emb_size + cell_size, cell_size], \n",
    "                          dtype=tf.float32,\n",
    "                          initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.1))\n",
    "    \n",
    "    network_dict['b_o'] = tf.get_variable('b_o', \n",
    "                          shape=[cell_size,], \n",
    "                          dtype=tf.float32,\n",
    "                          initializer=tf.zeros_initializer())\n",
    "\n",
    "create_lstm_variables(hyper_parameters, network_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_cell(word_embeddings, # (bs, embedding_size)\n",
    "              h_t_1, # (bs, hidden_size)\n",
    "              c_t_1, # (bs, hidden_size)\n",
    "              network_dict): # (bs, hidden_size)\n",
    "    W_i = network_dict['W_i']\n",
    "    b_i = network_dict['b_i']\n",
    "    W_o = network_dict['W_o']\n",
    "    b_o = network_dict['b_o']\n",
    "    W_f = network_dict['W_f']\n",
    "    b_f = network_dict['b_f']\n",
    "    W_c = network_dict['W_c']\n",
    "    b_c = network_dict['b_c']\n",
    "    concated_input = tf.concat([word_embeddings, h_t_1], axis=1) # (bs, hidden_size + embedding_size)\n",
    "    forget_gate = tf.nn.sigmoid(tf.matmul(concated_input, W_f) + b_f) # (bs, hidden_size)\n",
    "    input_gate = tf.nn.sigmoid(tf.matmul(concated_input, W_i) + b_i) # (bs, hidden_size)\n",
    "    update = tf.nn.tanh(tf.matmul(concated_input, W_c) + b_c) # (bs, hidden_size)\n",
    "    output_gate = tf.nn.sigmoid(tf.matmul(concated_input, W_o) + b_o) # (bs, hidden_size)\n",
    "    c_t = c_t_1 * forget_gate + (update * input_gate) # (bs, hidden_size)\n",
    "    h_t = tf.nn.tanh(c_t) * output_gate # (bs, hidden_size)\n",
    "    return h_t, c_t # (bs, hidden_size), (bs, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm(hyper_parameters, network_dict):\n",
    "    word_embeddings = network_dict['word_embeddings'] # (batch_size, max_len, embedding_size)\n",
    "    outputs = []\n",
    "    # create special constant for first timestep cell state\n",
    "    zero_state = tf.zeros(shape=(hyper_parameters['bs'], hyper_parameters['cell_size']))\n",
    "    c_t = zero_state\n",
    "    h_t = zero_state\n",
    "    # iterate over each word\n",
    "    for timestep in range(hyper_parameters['max_len']):\n",
    "        # get a slice for t-th word of each batch\n",
    "        one_word_batch = word_embeddings[:, timestep, :]  # (bs, embedding_size)\n",
    "        # compute lstm_cell\n",
    "        h_t, c_t = lstm_cell(one_word_batch, h_t, c_t, network_dict)\n",
    "        # remember the output for each step\n",
    "        outputs.append(h_t)\n",
    "    network_dict['lstm_outputs'] = outputs # list of size `max_len` of tensors (bs, hidden_size)\n",
    "    \n",
    "create_lstm(hyper_parameters, network_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(lstm_outputs):  100\n",
      "stm_outputs[0].shape:  (5, 512)\n"
     ]
    }
   ],
   "source": [
    "lstm_outputs = network_dict['lstm_outputs']\n",
    "print(\"len(lstm_outputs): \", len(lstm_outputs))\n",
    "print(\"stm_outputs[0].shape: \", lstm_outputs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loss_function(hyper_parameters, network_dict):\n",
    "    lstm_outputs = network_dict['lstm_outputs'] # list of size `max_len` of tensors (bs, hidden_size)\n",
    "    input_words_inds = network_dict['ph_X'] # (batch_size, max_example_len)\n",
    "    \n",
    "    input_words = input_words_inds[:, 1:] # shift all input words left by 1\n",
    "    # and add <pad> token index (1) at the end each example\n",
    "    ones_vector = tf.ones(shape=[hyper_parameters['bs'],1], dtype=tf.int32)\n",
    "    target_words =  tf.concat([input_words, ones_vector], axis = 1) # (bs, max_len)\n",
    "    \n",
    "    W = tf.get_variable('W_softmax', # softmax layer weights\n",
    "                      shape=[hyper_parameters['cell_size'], hyper_parameters['voc_size']], \n",
    "                      dtype=tf.float32,\n",
    "                      initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.1))\n",
    "    b = tf.get_variable('b_softmax', # softmax layer bias\n",
    "                        shape=[hyper_parameters['voc_size'],], \n",
    "                        dtype=tf.float32,\n",
    "                        initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.1))\n",
    "    \n",
    "    loss = tf.constant(0.0, dtype=tf.float32)\n",
    "    for t in range(hyper_parameters['max_len']):\n",
    "        logits = tf.matmul(lstm_outputs[t], W) + b # (bs, voc_size)\n",
    "        softmax_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, \n",
    "                                                                      labels=target_words[:, t]) # (bs,)\n",
    "        loss_t = tf.reduce_mean(softmax_loss) # batch_size, max_example_len\n",
    "        loss += loss_t\n",
    "    network_dict['loss'] = loss \n",
    "\n",
    "create_loss_function(hyper_parameters, network_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyper_parameters['lr'] = 0.001 # learning rate\n",
    "\n",
    "def create_training_operation(hyper_parameters, network_dict):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=hyper_parameters['lr'])\n",
    "    # some other options:\n",
    "    # optimizer = tf.train.GradientDescentOptimizer(learning_rate=hyper_parameters['lr'])\n",
    "    # optimizer = tf.train.MomentumOptimizer(learning_rate=hyper_parameters['lr'], momentum=0.9)\n",
    "    network_dict['train_op'] = optimizer.minimize(network_dict['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network(hyper_parameters):\n",
    "    tf.reset_default_graph()\n",
    "    network_dict = dict()\n",
    "    create_placeholders(hyper_parameters, network_dict)\n",
    "    create_embedding_layer(hyper_parameters, network_dict)\n",
    "    create_lstm_variables(hyper_parameters, network_dict)\n",
    "    create_lstm(hyper_parameters, network_dict)\n",
    "    create_loss_function(hyper_parameters, network_dict)\n",
    "    create_training_operation(hyper_parameters, network_dict)\n",
    "    return network_dict\n",
    "\n",
    "def describe_graph():\n",
    "    print(\"Trainable variables:\")\n",
    "    for var in tf.trainable_variables():\n",
    "        print(var.name, ':',  var.shape)\n",
    "    print()\n",
    "    total_parameters = np.sum([np.prod(var.shape.as_list()) for var in tf.trainable_variables()])\n",
    "    print(\"Total parameter count:\", total_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable variables:\n",
      "var_embs:0 : (10000, 256)\n",
      "W_i:0 : (768, 512)\n",
      "b_i:0 : (512,)\n",
      "W_f:0 : (768, 512)\n",
      "b_f:0 : (512,)\n",
      "W_c:0 : (768, 512)\n",
      "b_c:0 : (512,)\n",
      "W_o:0 : (768, 512)\n",
      "b_o:0 : (512,)\n",
      "W_softmax:0 : (512, 10000)\n",
      "b_softmax:0 : (10000,)\n",
      "\n",
      "Total parameter count: 9264912\n"
     ]
    }
   ],
   "source": [
    "hyper_parameters = {'bs': 5, \n",
    "                    'voc_size': voc_size,\n",
    "                    'cell_size': 512, \n",
    "                    'emb_size': 256, \n",
    "                    'max_len': 100, \n",
    "                    'lr': 0.001}\n",
    "\n",
    "network_dict = create_network(hyper_parameters)\n",
    "describe_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "def train_step(network_dict, input_batch, tf_session, do_backward=True):\n",
    "    loss = network_dict['loss']\n",
    "    ph_X = network_dict['ph_X']\n",
    "    if do_backward:\n",
    "        train_op = network_dict['train_op']\n",
    "        _, loss_value = tf_session.run([train_op, loss], feed_dict={ph_X: input_batch})\n",
    "    else:\n",
    "        loss_value = sess.run(loss, feed_dict={ph_X: input_batch})\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass took 1.83 seconds\n",
      "Loss = 930.821716\n",
      "\n",
      "One forward-backward pass took 12.77 seconds\n",
      "Loss = 930.821716\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "batch = get_sample_batch(hyper_parameters['bs'])\n",
    "batch_padded = pad_batch(batch, hyper_parameters['max_len'])\n",
    "\n",
    "t0 = time.time()\n",
    "loss_value = train_step(network_dict, batch_padded, sess, do_backward=False)\n",
    "print(\"Forward pass took %.2f seconds\" % float(time.time() - t0))\n",
    "print(\"Loss = %f\" % loss_value)\n",
    "print()\n",
    "\n",
    "t0 = time.time()\n",
    "loss_value = train_step(network_dict, batch_padded, sess, do_backward=True)\n",
    "print(\"One forward-backward pass took %.2f seconds\" % float(time.time() - t0))\n",
    "print(\"Loss = %f\" % loss_value)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
